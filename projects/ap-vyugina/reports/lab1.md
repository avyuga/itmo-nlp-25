# Лабораторная работа №1

## 1. Описание задания
В рамках лабораторной работы была реализована сегментация текста на предложения, а также токенизация текста на основе регулярных выражений. Также при токенизации нужна обработка более сложных кейсов, в данном случае - адреса электронной почты, телефонов, аббревиатур. Также добавлены простые кейсы - поиск url и валютных символов.


## 2. Результаты работы
Были написаны следующие регулярные выражения:
- email: `r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'`. Выделение происходит по `@` и расположенной далее точке.
- номер телефона: `r'(?:\+?[1-9]\d{0,3}[-.\s]?)?\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{2,4}(?:[-.\s]?[0-9]+)?'`. Первая группа отвечает за код страны (возможен +, цифры от 1 до 9 в количестве 0-3). Вторая группа - код города/региона, возможно в скобках. Далее одинаковые группы - пробел или тире, группы цифр. Таких групп три.
- url: `r'https?://[^\s<>"{}|\\^\[\]]+` - ключевое слово `https://`
- валюта: `r'\$[\d,]+(?:\.\d{2})?|\d+\.\d{2}\$'`. Работает только для `$`, но легко расширяется на любые другие символы
- аббревиатуры: `r'\b(?:Dr|Mr|Mrs|Ms|Prof|St|Inc|Corp|Ltd|Co)\.\s+[A-Z][a-zA-Z]*(?:\'[a-z]+)?'` - ищет аббревиатуры из заданного списка, затем проверяет, есть ли за ними слово.

### Последовательность действий
1. **Сегментация предложений:** 
    - замена точек после аббревиатур на токен
    - разделение по символу (точка, восклицвтельный/вопросительный знаки)
    - обратная замена токена на точку после аббревиатур
2. **Токенизация предложений:**
    - в тексте ищется паттерн (в заданном приоритете - от сложных паттернов с пробелами к простым)
    - слово, удовлетворяющее паттерну, попадает в токены целиком
    - оставшийся текст далее обрабатывается регулярными выражениями
    - простые слова, не попадающие в регулярки, очищаются от лишних пробелов. Слова попадают в список токенов в том случае, если они не попадают в список стоп-слов
3. **Лемматизация и стемминг:**
    - лемматизация через `nltk.stem.WordNetLemmatizer`
    - стемминг через `nltk.stem.SnowballStemmer`


### Производительность
```
Set: train [120000 lines] took 89.844 s
Set: test [7600 lines] took 5.133 s
```

## 3. Обработка сложных кейсов c омонимией

1. Два разных по смыслу слова приводятся к одинаковой начальной форме
```
Текст: My car needs new tires. I'm completely tired after working.
Токены: ['My', 'car', 'needs', 'new', 'tires', "I'm", 'completely', 'tired', 'working']
Стемминг: ['my', 'car', 'need', 'new', 'tire', "I'm", 'complet', 'tire', 'work']
Лемматизация: ['My', 'car', 'need', 'new', 'tire', "I'm", 'completely', 'tired', 'working']
```

2. Два слова с единым корнем приводятся к разным начальным формам
```
Текст: The carrier carried a huge bag.
Токены: ['The', 'carrier', 'carried', 'huge', 'bag']
Стемминг: ['the', 'carrier', 'carri', 'huge', 'bag']
Лемматизация: ['The', 'carrier', 'carried', 'huge', 'bag']
```

3. Слова в разных формах, но с единым корнем и смыслом приводятся к разным начальным формам
```
Текст: I flew to Japan for 10 hours, the flights were delayed. I hate flying!
Токены: ['I', 'flew', 'Japan', '10', 'hours', 'flights', 'delayed', 'I', 'hate', 'flying']
Стемминг: ['i', 'flew', 'japan', '10', 'hour', 'flight', 'delay', 'i', 'hate', 'fli']
Лемматизация: ['I', 'flew', 'Japan', '10', 'hour', 'flight', 'delayed', 'I', 'hate', 'flying']
```


